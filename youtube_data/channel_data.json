{
  "channel": {
    "title": "AI For Beginners",
    "description": "Welcome to the channel Artificial Intelligence For Beginners! \ud83e\udd16\u2728\n\nWe break down complex AI concepts into easy-to-understand videos, perfect for beginners and enthusiasts alike. Whether you're curious about machine learning, neural networks, or the future of AI, you'll find engaging and informative content here. \ud83c\udf93\ud83d\udcda\n\n\ud83d\udd0d What We Cover:\n\n#machinelearning #ai #neuralnetworks #artificialintelligence #statistics #deeplearning #datascience #aieducation  #techinnovation #aitrends #aiexplained #aiforbeginners #mlalgorithms #aiapplications #aibasics #aiforall \n\n\ud83d\udca1 Why To Subscribe?\n\n\u2022 Simplified explanations,\n\u2022 Real-world examples,\n\u2022 Experts sharing valuable knowledge,\n\u2022 Diverse topics,\n\u2022 AI is made more accessible than ever.\n\nJoin our community of learners and let's explore the fascinating world of artificial intelligence together! \ud83c\udf10\ud83e\udde0\nDon't forget to like, subscribe, and hit the bell icon for updates on our latest videos! \ud83d\udd14\ud83d\udc4d\n",
    "thumbnail": "https://yt3.ggpht.com/s1UaRGPDZxzAwLUVe7lxnUGIcrvtQApzJVgsYcQXq36DUtdw-SD52M-eG6jBCz3V0FwPbepiUg=s800-c-k-c0x00ffffff-no-rj",
    "subscribers": "16600",
    "views": "300379",
    "videos": "29",
    "total_likes": 11510
  },
  "videos": [
    {
      "title": "Most People Fail This Simple Probability Puzzle | Monty Hall Problem Explained!",
      "description": "\ud83d\udd25 #artificialintelligence #ai #ml #education #probability #statistics #science #puzzle #machinelearning \nThe Monty Hall Problem is one of the most counterintuitive and famous puzzles in probability theory. In this video, I discuss the classic game show scenario where you're faced with a simple choice: three doors, one hiding a car and the other two hiding goats. You pick a door. Then, the host - Monky hall - opens one of the remaining doors to reveal a goat. Now the dilemma is: should you stick with your original choice or switch? At first it sounds like a 50/50 decision, but it's not.\n\nI explain the trick behind the problem in three different simple ways, including the classical, frequentist approach and the Bayesian method. By the end of this video, you will understand why switching actually doubles your chances of winning the car. The idea behind switching the doors is hard to understand at first. Why would it matter? The probability can be incredibly misleading. If you understand it, you can have a great advantage!\n\n\ud83d\udd0d Key points covered:\n\n0:00 - Introduction to the Monty Hall Problem.\n0:37 - The answer to the problem is...\n1:11 - Explanation N1.\n1:39 - Explanation N2.\n1:55 - What if there are 20 doors?\n2:15 - Explanation N3. Bayesian Approach.\n4:00 - Subscribe!\n\n\ud83c\udfb5 Music in the video:\nKool Kats by Kevin MacLeod is licensed under a Creative Commons Attribution 4.0 license. https://creativecommons.org/licenses/by/4.0/\n\n\ud83d\udd14 Don't forget to like, subscribe, and hit the bell icon to stay updated with our latest videos! \n\n\ud83e\udd16 Note that we use synthetic generations, such as AI-generated images and voices, to enhance the appeal and engagement of our content.\n\n\ud83c\udf10 If you have any questions or topics you want us to cover, leave a comment below. Additionally, share with your thoughts about the content, how do you think we can make them better? Thanks for watching!",
      "thumbnail": "https://i.ytimg.com/vi/6GW9ts0q3nY/mqdefault.jpg",
      "videoId": "6GW9ts0q3nY",
      "publishedAt": "2025-05-29T06:01:32Z",
      "likes": 22,
      "views": 678
    },
    {
      "title": "Central Limit Theorem Intuition Explained Like You're 5!",
      "description": "#ai #ml #artificialintelligence #machinelearning #statistics #stats #science #education #clt #datascience \n\ud83d\udd25 Central Limit Theorem is a fundamental theorem in statistics. It states that as the sample size increases the sampling distribution of the sample means approaches a normal distribution. This enables us to conduct hypothesis testing, construct confidence intervals, and draw conclusions!\nThere are other conditions that are important for the CLT to be true. Specifically, your sample should be independent and identically distributed (i.i.d.) with a finite variance. If all conditions are met, you can draw inferences about the population mean even if the population distribution is not normally distributed.\nWhen the conditions are not met, bootstrapping can be used to approximate the sampling distribution of the sample means.\n\nWhile the video explained the high-level intuition behind the central limit theorem, there are some important considerations to take into account.\nFollow us to learn more!\n\nInstagram:\nhttps://www.instagram.com/easyaiforall/\n\n\ud83d\udd0d Key points covered:\n\n0:00 - Introduction\n0:30 - Population and Sampling\n1:06 - Population and Sampling: Example\n1:38 - Misconception about CLT\n2:02 - Correct CLT\n3:00 - Bootstrapping\n3:35 - CLT vs. Bootstrapping\n3:51 - Confidence Intervals and Hypothesis testing\n4:00 - Subscribe to us!\n\n\ud83c\udfb5 Music used in the video:\n\nLocal Elevator by Kevin MacLeod is licensed under a Creative Commons Attribution 4.0 license. https://creativecommons.org/licenses/by/4.0/\n\n\ud83d\udd14 Don't forget to like, subscribe, and hit the bell icon to stay updated with our latest videos! \n\n\ud83e\udd16 Note that we use synthetic generations, such as AI-generated images and voices, to enhance the appeal and engagement of our content.\n\n\ud83c\udf10 If you have any questions or topics you want us to cover, leave a comment below. Additionally, share with your thoughts about the content, how do you think we can make them better? Thanks for watching!",
      "thumbnail": "https://i.ytimg.com/vi/3lu7H6EwFSU/mqdefault.jpg",
      "videoId": "3lu7H6EwFSU",
      "publishedAt": "2025-04-03T07:15:19Z",
      "likes": 80,
      "views": 2064
    },
    {
      "title": "Best Practices for Effective Data Visualization In Machine Learning!",
      "description": "#ai #ml #datascience #datavisualization #presentation #artificialintelligence #machinelearning #education #visualization \n\ud83d\udd25 Data visualization is an important component of a successful data science, machine learning or deep learning project. It makes easier to understand patterns, relationships, detect trends that might not be obvious in raw numbers. Most importantly, it improves communication, allowing technical and non-technical audiences to understand the story behind the visual elements. In this video, I will go through some types of charts (for univariate and multivariate analysis) and discuss when and how to use them effectively. Additionally, good visualizations stand out by readable axis labels, inclusion of legends, and proper titles.\n\nBe sure to follow us on Instagram, where you can find a quiz with questions for this video! Keep track of your progress and challenge yourself!\n\nInstagram:\nhttps://www.instagram.com/easyaiforall/\n\n\ud83d\udd0d Key points covered:\n\n0:00 - Introduction.\n0:39 - What is the purpose of data visualization?\n1:23 - Know your audience first!\n1:47 - Univariate analysis, bar charts.\n2:05 - Univariate analysis, pie charts.\n2:35 - Univariate analysis, line charts.\n2:49 - Univariate analysis, histograms.\n3:10 - Univariate analysis, density plots.\n3:23 - Univariate analysis, boxplots.\n3:55 - Univariate analysis, violin plots.\n4:15 - Multivariate analysis, bar charts.\n4:44 - Multivariate analysis, stacked bar charts.\n5:03 - Multivariate analysis, overlapping density plots.\n5:24 - Main Types of Palettes.\n5:34 - Categorical Color Schemes.\n6:07 - Sequential Color Schemes.\n6:25 - Diverging Color Schemes.\n6:45 - Natural Color Choices.\n6:55 - Axis Labels.\n7:12 - Titles and Legends.\n7:33 - Subscribe to us!\n\n\n\ud83d\udd14 Don't forget to like, subscribe, and hit the bell icon to stay updated with our latest videos! \n\n\ud83e\udd16 Note that we use synthetic generations, such as AI-generated images and voices, to enhance the appeal and engagement of our content.\n\n\ud83c\udf10 If you have any questions or topics you want us to cover, leave a comment below. Additionally, share with your thoughts about the content, how do you think we can make them better? Thanks for watching!",
      "thumbnail": "https://i.ytimg.com/vi/8SFUXDHVLvg/mqdefault.jpg",
      "videoId": "8SFUXDHVLvg",
      "publishedAt": "2025-02-24T08:00:12Z",
      "likes": 155,
      "views": 3023
    },
    {
      "title": "6 Mistakes to Avoid When Learning Machine Learning in 2025",
      "description": "#ai #ml #artificialintelligence #machinelearning #datascience #learn #education #science \n\ud83d\udd25 There are critical mistakes that one should avoid when learning machine learning. In this video, we will explore 6 of them. Firstly, don't rush into advanced topic directly. Missing fundamental knowledge will harm you in the long run. Secondly, don't be discouraged if something doesn't work out! ML is  complicated and it takes time until you see a progress. Third, ask challenging and controversial questions to ML experts. This will help both sides to learn. Fourth, be transparent and hones about your projects. Never exaggerate or cheat when reporting your project. Don't steal other people's work. Lastly, don't repeat the most popular projects again and again. Instead of doing Titanic Survival Classification or MNIST handwritten digits classification, try to find a more interesting dataset that less people have worked on. \n\nThere are still lots of other mistakes that are important to avoid. If interested, subscribe to our channel and press the like button if you liked the video!\n\nBe sure to follow us on Instagram, where you can find a quiz with questions for this video! Keep track of your progress and challenge yourself!\n\nInstagram:\nhttps://www.instagram.com/easyaiforall/\n\n\ud83d\udd0d Key points covered:\n\n0:00 - Introduction.\n0:23 - Don't rush into advanced topics directly.\n1:28 - Don't be discouraged if something doesn't work out.\n1:59 - Ask questions to experts.\n2:28 - Always be transparent and honest.\n2:44 - Don't repeat overdone projects.\n3:08 - Subscribe to us!\n\n\ud83d\udd14 Don't forget to like, subscribe, and hit the bell icon to stay updated with our latest videos! \n\n\ud83e\udd16 Note that we use synthetic generations, such as AI-generated images and voices, to enhance the appeal and engagement of our content.\n\n\ud83c\udf10 If you have any questions or topics you want us to cover, leave a comment below. Additionally, share with your thoughts about the content, how do you think we can make them better? Thanks for watching!",
      "thumbnail": "https://i.ytimg.com/vi/yLlgHKaMyIw/mqdefault.jpg",
      "videoId": "yLlgHKaMyIw",
      "publishedAt": "2025-02-06T07:49:46Z",
      "likes": 406,
      "views": 11533
    },
    {
      "title": "All Machine Learning Models Clearly Explained!",
      "description": "#ml #machinelearning #ai #artificialintelligence #datascience #regression #classification\n\ud83d\udd25 In this video, we explain every major Machine Learning algorithm. \nRegression models: Linear Regression, Polynomial Regression. \nClassification models: Logistic Regression, Naive Bayes. \nModels used for both: Decision Tree, Random Forest, Support Vector Machines, K-Nearest Neighbors. \nEnsembles: Bagging, Boosting, Voting and Stacking. \nDeep Learning: Fully Connected (Dense) Neural Networks. \nUnsupervised learning: K-Means clustering and Principal Component Analysis (PCA) dimensionality reduction technique.\n\nHeads up! You can't learn Machine Learning in just 22 minutes, a day, a week or even in a month! It needs a continuous dedication, patience, and consistent effort. I\u2019m here to guide you every step of the way with clear explanations, tips, and resources to make your learning experience easier! Don't worry if there were concepts that were hard to understand! \n\nKeep at it, and you\u2019ll get there. Subscribe and like the video if you found it helpful!\n\nStarting with this video, we\u2019ll be posting a quick quiz on our Instagram page to help you review the material and test your understanding! It\u2019s a great way to reinforce what you\u2019ve learned and see how well you\u2019re understanding the concepts. Be sure to follow us on Instagram, keep track of your progress and challenge yourself!\n\nInstagram:\nhttps://www.instagram.com/easyaiforall/\n\n\ud83d\udd0d Key points covered:\n\n0:00 - Introduction.\n0:22 - Linear Regression.\n2:00 - Logistic Regression.\n3:12 - Naive Bayes.\n4:15 - Decision Trees.\n6:25 - Random Forests.\n7:55 - Support Vector Machines.\n10:05 - K-Nearest Neighbors.\n12:23 - Ensembles.\n12:49 - Ensembles (Bagging).\n13:18 - Ensembles (Boosting).\n13:55 - Ensembles (Voting).\n14:48 - Ensembles (Stacking).\n15:55 - Neural Networks.\n18:59 - K-Means.\n20:58 - Principal Component Analysis.\n22:05 - Subscribe to us!\n\n\ud83d\udd14 Don't forget to like, subscribe, and hit the bell icon to stay updated with our latest videos! \n\n\ud83e\udd16 Note that we use synthetic generations, such as AI-generated images and voices, to enhance the appeal and engagement of our content.\n\n\ud83c\udf10 If you have any questions or topics you want us to cover, leave a comment below. Additionally, share with your thoughts about the content, how do you think we can make them better? Thanks for watching!",
      "thumbnail": "https://i.ytimg.com/vi/0YdpwSYMY6I/mqdefault.jpg",
      "videoId": "0YdpwSYMY6I",
      "publishedAt": "2025-02-02T18:35:04Z",
      "likes": 8614,
      "views": 226754
    },
    {
      "title": "How is Artificial Intelligence different from Traditional Programming?",
      "description": "\ud83d\udd25 People often conflate artificial intelligence with traditional programming, but while they share some similarities, they differ significantly. In traditional programming, the programmer explicitly defines all the logic and instructions the system follows. In contrast, artificial intelligence uses specialized algorithms to derive answers, instructions, or logic from data.\n\nIn this video, we focus on the term Machine Learning. This is because artificial intelligence encompasses branches that don\u2019t necessarily \u201clearn\u201d but rely on predefined algorithms or rules to make decisions.\n\n\ud83d\udd0d Key points covered:\n\n0:00 - Introduction.\n0:10 - Both use programming languages.\n0:44 - Traditional Programming.\n1:37 - When traditional programming is not a good option.\n2:30 - Machine Learning.\n3:15 - Conclusion.\n3:30 - Subscribe to us!\n\n\ud83d\udd14 Don't forget to like, subscribe, and hit the bell icon to stay updated with our latest videos! \n\n\ud83e\udd16 Note that we use synthetic generations, such as AI-generated images and voices, to enhance the appeal and engagement of our content.\n\n\ud83c\udf10 If you have any questions or topics you want us to cover, leave a comment below. Additionally, share with your thoughts about the content, how do you think we can make them better? Thanks for watching!",
      "thumbnail": "https://i.ytimg.com/vi/cnEsy_428fM/mqdefault.jpg",
      "videoId": "cnEsy_428fM",
      "publishedAt": "2024-12-27T23:48:22Z",
      "likes": 144,
      "views": 4055
    },
    {
      "title": "The Ultimate Guide to Hyperparameter Tuning | Grid Search vs. Randomized Search",
      "description": "#ai #ml #datascience #learnai #learning #artificialintelligence #machinelearning \n\ud83d\udd25 Hyperparameters are the parameters of the model that are not learned during the training process but are set by the user before the process starts. They control the training phase and model behavior. Different machine learning models have different hyperparameters that can have a significant affect on the performance of the model. There are two common ways for hyperparameter search. Using grid search you define potential values for each hyperparameter and train a separate model for each set. This method is computationally expensive, because you train too many models as the number of hyperparameters and number of their values increases. Using Randomized Search, on the other hand, you provide range for each hyperparameter and how many times you want to train a model. The method then samples values from the range of each hyperparameter and trains separate models (as many as you specified). Both have their advantages and disadvantages.\n\n\ud83d\udd0d Key points covered:\n\n0:00 - What are the hyperparameters?\n0:25 - Why are hyperparameters important?\n0:35 - Example of a hyperparameter.\n1:07 - But how to find the best hyperparameters?\n1:26 - Grid Search.\n2:09 - One major problem of grid search.\n2:31 - Randomized Search.\n3:04 - Which one to choose and when?\n3:19 - What about large neural networks?\n3:31 - Subscribe to us!\n\n\ud83d\udd14 Don't forget to like, subscribe, and hit the bell icon to stay updated with our latest videos! \n\n\ud83e\udd16 Note that we use synthetic generations, such as AI-generated images and voices, to enhance the appeal and engagement of our content.\n\n\ud83c\udf10 If you have any questions or topics you want us to cover, leave a comment below. Additionally, share with your thoughts about the content, how do you think we can make them better? Thanks for watching!",
      "thumbnail": "https://i.ytimg.com/vi/lfiw2Rh2v8k/mqdefault.jpg",
      "videoId": "lfiw2Rh2v8k",
      "publishedAt": "2024-12-01T07:25:43Z",
      "likes": 176,
      "views": 5616
    },
    {
      "title": "Normalization and Standardization | Why to Scale the Features? | ML Basics",
      "description": "#ai #ml #artificialintelligence #learning #coding #machinelearning #datascience \n\ud83d\udd25 Normalization and Standardization are the most popular scaling methods used in Machine Learning. But why we scale our features? Raw data often comes with features having varying scales. If we decide to use algorithms relying on distance calculations like K-Nearest Neighbors, Support Vector Machines or even Neural Networks, we will need to scale our features to be sure the feature having higher scale does not dominate the calculations. Normalization scales the feature to a fixed range 0 to 1, or -1 to 1. Standardization, on the other hand, is useful when your data follows a normal distribution, because it standardizes the distribution forcing it to have 0 mean and 1 standard deviation. Thus, it centers the data and standardizes the spread. \n\nBoth methods have their advantages and disadvantages. In practice, the proper method is selected by taking into account the data distribution, the algorithm, range sensitivity, etc.\n\n\ud83d\udd0d Key points covered:\n\n0:00 - Introduction.\n0:28 - What is data scaling?\n0:38 - Why we need data scaling?\n1:10 - Min Max Scaling = Normalization.\n1:26 - An example of Min Max Scaling.\n1:42 - It is very sensitive to outliers!\n1:56 - Standardization.\n2:16 - Standardization is also sensitive to outliers!\n2:45 - End notes.\n2:58 - Subscribe to us!\n\n\ud83d\udd14 Don't forget to like, subscribe, and hit the bell icon to stay updated with our latest videos! \n\n\ud83e\udd16 Note that we use synthetic generations, such as AI-generated images and voices, to enhance the appeal and engagement of our content.\n\n\ud83c\udf10 If you have any questions or topics you want us to cover, leave a comment below. Additionally, share with your thoughts about the content, how do you think we can make them better? Thanks for watching!",
      "thumbnail": "https://i.ytimg.com/vi/dHSWjZ_quaU/mqdefault.jpg",
      "videoId": "dHSWjZ_quaU",
      "publishedAt": "2024-11-07T08:00:51Z",
      "likes": 77,
      "views": 2525
    },
    {
      "title": "Easiest Guide to K-Fold Cross Validation | Explained in 2 Minutes!",
      "description": "#ai #ml #artificialintelligence #education #learning #datascience \n\ud83d\udd25 K-Fold Cross Validation explained in 2 minutes! In this video, we talk about one of the best methods for hyperparameter tuning and assessing the generalizability of the model called K-Fold Cross-Validation. Unlike simple train-test-validation split, this method runs the training process with different subsets for training and validating, and then averages the results, making the performance estimation more reliable. While it has lots of advantages, it is not perfect! Suppose you have a large dataset and a complex model. It is too costly to train a complex model with huge data several times! In such cases, train-test-validation split is a good approach, as for large datasets, random splitting becomes more reliable!\n\nSo, remember, go with simple train-test-validation split for larger datasets and go with K-Fold Cross-Validation for smaller ones. Also, explore other methods for Cross-Validation like Stratified Cross-Validation, Leave-P-Out Cross-Validation, etc.\n\n\ud83d\udd0d Key points covered:\n\n0:00 - Introduction.\n0:11 - Train-Test-Validation split.\n0:40 - The main assumption of train-test-validation split.\n1:10 - What about small datasets?\n1:18 - K-Fold Cross Validation Explained.\n1:48 - Advantages of K-Fold Cross Validation.\n2:05 - Disadvantage of K-Fold Cross Validation.\n2:15 - Large vs. Small Datasets.\n2:25 - Do this to get up to 50% additional accuracy score! :)\n2:31 - Subscribe to us!\n\n\ud83d\udd14 Don't forget to like, subscribe, and hit the bell icon to stay updated with our latest videos! \n\n\ud83e\udd16 Note that we use synthetic generations, such as AI-generated images and voices, to enhance the appeal and engagement of our content.\n\n\ud83c\udf10 If you have any questions or topics you want us to cover, leave a comment below. Additionally, share with your thoughts about the content, how do you think we can make them better? Thanks for watching!",
      "thumbnail": "https://i.ytimg.com/vi/k4vi6Q299Ng/mqdefault.jpg",
      "videoId": "k4vi6Q299Ng",
      "publishedAt": "2024-10-28T04:51:20Z",
      "likes": 68,
      "views": 2050
    },
    {
      "title": "7 PROVEN Strategies To Become An AI Engineer (2025 Updated)",
      "description": "#ai #ml #engineering #datascience #data #aiengineer #education\n\ud83d\udd25 Want to become an AI Engineer but don't know how? In this video we present the best roadmap for entering the AI job market! The proven roadmap that fills all the gaps for new learners! Learning AI became a lot easier with the vast resources available on the internet, so we need to take the advantage of it! The most important thing is to follow the correct roadmap. Watch the video till the end, not to miss any of the important steps!\n\n\ud83d\udd0d Key points covered:\n\n0:00 - Introduction.\n0:48 - Step 1 - Learn Python.\n1:20 - Step 2 - Learn Math.\n1:48 - Step 3 - Optional, Learn DSA.\n2:28 - Step 4 - Learn The ML Basics.\n3:07 - Step 5 - Learn Deep Learning.\n3:42 - Step 6 - Choose Your Direction.\n4:01 - Step 7 - The Most Important Step!\n4:18 - Subscribe to us!\n\n\nMaterials from the video:\n\nLearning Python: Powerful Object Oriented Programming. By Mark Lutz\nhttps://cfm.ehu.es/ricardo/docs/python/Learning_Python.pdf\n\nData Structures & Algorithms in Python. By Michael T. Goodrich and others\nhttps://github.com/manishbisht/Competitive-Programming/blob/master/Resources/books/\n\nIntroduction to Algorithms. By Thomas Cormen and others\nhttps://github.com/calvint/AlgorithmsOneProblems/tree/master/Algorithms\n\nMath for Programmers. By Paul Orland\nhttps://wangwei1237.github.io/shares/Math-for-Programmers.pdf\n\nMathematics for Machine Learning. By Marc Peter Deisenroth and others\nhttps://mml-book.github.io/book/mml-book.pdf\n\nPython Machine Learning. By Sebastian Raschka and Vahid Mirjalili\nhttp://radio.eng.niigata-u.ac.jp/wp/wp-content/uploads/2020/06/python-machine-learning-2nd.pdf\n\nIntroduction to Machine Learning with Python. By Andreas C. M\u00fcller and others\nhttps://www.nrigroupindia.com/e-book/Introduction%20to%20Machine%20Learning%20with%20Python%20(%20PDFDrive.com%20)-min.pdf\n\nDeep Learning Specializations\nhttps://www.coursera.org/specializations/deep-learning\n\nMachine Learning Specialization\nhttps://www.coursera.org/specializations/machine-learning-introduction\n\nMathematics for Machine Learning and Data Science Specialization\nhttps://www.coursera.org/specializations/mathematics-for-machine-learning-and-data-science\n\n\n\ud83d\udd14 Don't forget to like, subscribe, and hit the bell icon to stay updated with our latest videos! \n\n\ud83e\udd16 Note that we use synthetic generations, such as AI-generated images and voices, to enhance the appeal and engagement of our content.\n\n\ud83c\udf10 If you have any questions or topics you want us to cover, leave a comment below. Additionally, share with your thoughts about the content, how do you think we can make them better? Thanks for watching!",
      "thumbnail": "https://i.ytimg.com/vi/A6grnwE9sWw/mqdefault.jpg",
      "videoId": "A6grnwE9sWw",
      "publishedAt": "2024-10-16T09:26:24Z",
      "likes": 649,
      "views": 12703
    },
    {
      "title": "3 Main Types of Missing Data | Do THIS Before Handling Missing Values!",
      "description": "#ai #ml #datascience #data #machinelearning #artificialintelligence \n\ud83d\udd25 This video covers the three main types of missing values: missing completely at random, missing at random and missing not at random. Before moving to the missing value handling step, you need to understand where are the values in the dataset? Why they disappeared? \n\nYou can proceed to the missing value handling after understanding the statistical effect of the missing data points on your analysis. What if you mistakenly delete an important information which can lead to an underestimation? We bring valuable examples to clearly explain main differences among these three categories.\n\nRemember, missing completely at random occurs when the missing data is completely random and does not relate with the observed data. Missing at random refers to those missing values that are related to the observed data. While missing not at random is the most problematic. In that case, reasons are tied to the characteristics of the missing data, making it difficult or impossible to directly infer or predict what those missing values might be based solely on the observed data.\n\n\ud83d\udd0d Key points covered:\n\n0:00 - Introduction.\n0:28 - In this video...\n0:34 - Types of missing data.\n0:42 - Missing Completely at Random.\n1:26 - Missing at Random.\n2:03 - Missing Not at Random.\n2:47 - Subscribe to us!\n\n\ud83d\udd14 Don't forget to like, subscribe, and hit the bell icon to stay updated with our latest videos! \n\n\ud83e\udd16 Note that we use synthetic generations, such as AI-generated images and voices, to enhance the appeal and engagement of our content.\n\n\ud83c\udf10 If you have any questions or topics you want us to cover, leave a comment below. Additionally, share with your thoughts about the content, how do you think we can make them better? Thanks for watching!",
      "thumbnail": "https://i.ytimg.com/vi/UzsWr9X98J8/mqdefault.jpg",
      "videoId": "UzsWr9X98J8",
      "publishedAt": "2024-10-05T08:52:13Z",
      "likes": 87,
      "views": 1997
    },
    {
      "title": "Numerical vs. Categorical Data | Represent Your Dataset Correctly!",
      "description": "#ai #ml #datascience #aiexplained #artificialintelligence #categorical #numerical #python #computerscience #data \n\ud83d\udd25 This video defines two main data types in tabular data: numerical and categorical. Numerical data represents numbers both discrete and continuous. Discrete data consists of whole numbers, often they represent counting things. Continuous data contains any numbers including decimals and fractions. Categorical data is the trickiest one. You may have either nominal (unordered) or ordinal (ordered) categories. In ordinal data categories are ordered, so you just need to map higher integers to categories higher in the hierarchy. While in the unordered (nominal) case, you need to use a technique called one-hot encoding. One-hot encoding defines a new binary variable for each category in the variable. \n\nIt is important to note that one-hot encoding can result in a bunch of new columns, which can significantly expand the dimensionality (number of columns) of the data. You have to think twice before applying one-hot encoding directly to a variable that has many unique variables. More on this we will talk in the upcoming videos!\n\n\ud83d\udd0d Key points covered:\n\n0:00 - Introduction.\n0:13 - Numerical data.\n0:20 - Continuous data.\n0:27 - Discrete data.\n0:38 - Categorical data.\n0:53 - Binary encoding.\n1:07 - Non-binary case.\n1:15 - Ordinal data.\n1:30 - Nominal data.\n1:42 - One-hot encoding.\n2:08 - Subscribe to us!\n\n\ud83d\udd14 Don't forget to like, subscribe, and hit the bell icon to stay updated with our latest videos! \n\n\ud83e\udd16 Note that we use synthetic generations, such as AI-generated images and voices, to enhance the appeal and engagement of our content.\n\n\ud83c\udf10 If you have any questions or topics you want us to cover, leave a comment below. Additionally, share with your thoughts about the content, how do you think we can make them better? Thanks for watching!",
      "thumbnail": "https://i.ytimg.com/vi/3-R6PikEu5A/mqdefault.jpg",
      "videoId": "3-R6PikEu5A",
      "publishedAt": "2024-09-24T07:06:56Z",
      "likes": 34,
      "views": 1061
    },
    {
      "title": "8 Best Tips For Cleaning Your Data | Data Cleaning | Machine Learning, Data Preparation.",
      "description": "\ud83d\udd25 Data cleaning is a crucial step in data science and machine learning. Proper data cleaning is often more than 90% of any project in machine learning. Why? Because especially machine learning (statistical) models work well when they are trained with meaningful features! But before constructing meaningful features you need to clean your data. Meaning, correct data types, handle missing and incorrect values (like typos, inconsistencies, etc.). Remove columns that add noise (unique identifier columns). In this video, we will show you 8 best tips and suggestions how to properly clean your dataset. \n\nNote! While we provide the most important and common ways for data cleaning, remember that it is a specific task and each problem requires specific approach. Feel free to ask questions in the comments sections regarding data cleaning!\n\n\ud83d\udd0d Key points covered:\n\n0:00 - Introduction.\n0:42 - Tip 1: Be sure to have one complete dataset.\n0:46 - Tip 2: Tidy data rules: Columns, Rows, Values.\n0:58 - Tip 3: Check for unique identifiers.\n1:10 - Tip 4: Have consistent column names.\n1:22 - Tip 5: Remove duplicates.\n1:29 - Tip 6: Look for typos, inconsistencies.\n1:35 - Tip 7: Check for missing values.\n1:40 - Tip 8: Check the data types.\n1:51 - Data cleaning is a specific task!\n2:11 - Subscribe to us!\n\n\ud83d\udd14 Don't forget to like, subscribe, and hit the bell icon to stay updated with our latest videos! \n\n\ud83e\udd16 Note that we use synthetic generations, such as AI-generated images and voices, to enhance the appeal and engagement of our content.\n\n\ud83c\udf10 If you have any questions or topics you want us to cover, leave a comment below. Additionally, share with your thoughts about the content, how do you think we can make them better? Thanks for watching!",
      "thumbnail": "https://i.ytimg.com/vi/J83qfwt-beQ/mqdefault.jpg",
      "videoId": "J83qfwt-beQ",
      "publishedAt": "2024-09-10T03:59:00Z",
      "likes": 55,
      "views": 1375
    },
    {
      "title": "How to Evaluate Your ML Models Effectively? | Evaluation Metrics in Machine Learning!",
      "description": "\ud83d\udd25 In this video we refer to the evaluation metrics used in machine learning. Confusion matrix, Accuracy, Precision, Recall and F1-Score are the most popular metrics for classification tasks. We explain the difference of each metric on a single example, showing that accuracy is well suited for balanced datasets, while other three for imbalanced ones. In some specific cases, we may prefer recall over precision and vice versa, or we might want to have both high using F1-Score. \n\nAdditionally, there are other important metrics like AUC and ROC. Metrics for unsupervised learning and regression tasks are different. These are more complex topics, which we will cover separately, so stay with us!\n\n\ud83d\udd0d Key points covered:\n\n0:00 - Introduction to the problem.\n0:20 - Understanding the confusion matrix.\n0:45 - Accuracy.\n0:59 - When not to use the accuracy?\n1:35 - Recall and Precision.\n1:45 - Precision.\n1:52 - Recall.\n2:02 - F1-Score.\n2:17 - How to choose between the metrics?\n2:25 - Important notes.\n2:45 - Subscribe to us!\n\n\ud83d\udd14 Don't forget to like, subscribe, and hit the bell icon to stay updated with our latest videos! \n\n\ud83e\udd16 Note that we use synthetic generations, such as AI-generated images and voices, to enhance the appeal and engagement of our content.\n\n\ud83c\udf10 If you have any questions or topics you want us to cover, leave a comment below. Additionally, share with your thoughts about the content, how do you think we can make them better? Thanks for watching!",
      "thumbnail": "https://i.ytimg.com/vi/FXokoJUhsLQ/mqdefault.jpg",
      "videoId": "FXokoJUhsLQ",
      "publishedAt": "2024-09-03T03:34:12Z",
      "likes": 74,
      "views": 2394
    },
    {
      "title": "The Role of Loss Functions | Most Common Loss Functions in Machine Learning | Explained!",
      "description": "\ud83d\udd25 Loss Functions are a key topic in machine learning. Those functions provide a metric of how good your model is performing. Usually, it is a number going from 0 to infinity. You try to optimize the model in a way, that it eventually reaches 0 (which never happens). Mostly loss functions are used either for regression or classification tasks. Regression losses include mean absolute error (MAE), mean squared error (MSE) or other functions that compare continuous values. For classification, on the other hand, we have cross-entropy that is the most popular one. Keep in mind that some ML algorithms use their own loss functions, specific to the requirements of the model. For instance, SVM uses the Hinge loss.\n\nThere are still some important questions to ask, like, why we can't use MSE for classification tasks? Why can't we think of our own loss function? So, subscribe to get the answers to these questions!\n\n\ud83d\udd0d Key points covered:\n\n0:00 - Introduction.\n0:04 - What is loss function?\n0:13 - Types of loss functions.\n0:17 - Regression loss functions (MAE, MSE)\n0:39 - MSE vs. MAE.\n1:02 - Classification loss functions (cross-entropy)\n1:16 - Binary cross-entropy: an example.\n1:27 - Multi-class classification (cross-entropy).\n1:33 - An example for multi-class case.\n1:46 - Some ML algorithms have their own losses.\n1:58 - Future topics.\n2:01 - Subscribe to us!\n\n\n\ud83d\udd14 Don't forget to like, subscribe, and hit the bell icon to stay updated with our latest videos! \n\n\ud83e\udd16 Note that we use synthetic generations, such as AI-generated images and voices, to enhance the appeal and engagement of our content.\n\n\ud83c\udf10 If you have any questions or topics you want us to cover, leave a comment below. Additionally, share with your thoughts about the content, how do you think we can make them better? Thanks for watching!",
      "thumbnail": "https://i.ytimg.com/vi/AUmZGGm6quw/mqdefault.jpg",
      "videoId": "AUmZGGm6quw",
      "publishedAt": "2024-08-30T02:22:32Z",
      "likes": 47,
      "views": 1015
    },
    {
      "title": "Main Types of Gradient Descent | Batch, Stochastic and Mini-Batch Explained! | Which One to Choose?",
      "description": "\ud83d\udd25 There are three main types of gradient descent: Batch, Stochastic and Mini-Batch. Batch gradient descent takes all observations for gradient computation, which is both accurate and resource heavy. Stochastic takes only one random observation from the data which is a poor approximation but introduces randomness. Mini-Batch is the mix of two, takes a random sample from the data. \n\nEach type has its own advantages and disadvantages. Batch gradient descent requires more resources and converges confidently to a minima (sometimes to a local minima), while stochastic converges faster due to frequent updates but fails to converge to exact minima (hovers around it). Additionally, randomness can help explore the parameter space even deeper. Mini-Batch is a compromise among those two and the most popular one! Remember, each problem has a separate approach, experiment to see which one works best for you!\n\n\ud83d\udd0d Key points covered:\n\n0:00 - Introduction.\n0:10 - Batch gradient descent.\n0:20 - Stochastic gradient descent.\n0:34 - Mini-batch gradient descent.\n0:48 - Batch gradient descent pros and cons.\n1:21 - Stochastic gradient descent pros and cons.\n1:51 - Mini-batch gradient descent pros and cons.\n2:10 - Subscribe to us!\n\n\ud83d\udd14 Don't forget to like, subscribe, and hit the bell icon to stay updated with our latest videos! \n\n\ud83e\udd16 Note that we use synthetic generations, such as AI-generated images and voices, to enhance the appeal and engagement of our content.\n\n\ud83c\udf10 If you have any questions or topics you want us to cover, leave a comment below. Additionally, share with your thoughts about the content, how do you think we can make them better? Thanks for watching!",
      "thumbnail": "https://i.ytimg.com/vi/cAfS2Uq6ujc/mqdefault.jpg",
      "videoId": "cAfS2Uq6ujc",
      "publishedAt": "2024-08-22T19:52:38Z",
      "likes": 31,
      "views": 724
    },
    {
      "title": "Gradient Descent Explained | How Do ML and DL Models Learn? | Simple Explanation!",
      "description": "\ud83d\udd25 In this video we cover gradient descent - an optimization algorithm used to train most of the ML and DL models. During the training process, the algorithm computes the gradient of the error function, which shows the magnitude and the direction to update the weights to reduce the error. Calculus is the core of gradient descent. Remember that in complex scenarios, we can't surely say if we found the global best solution or just a good one.\n\nIn the next video we will cover the main types of gradient descents and their pros and cons. Subscribe for more!\n\n\ud83d\udd0d Key points covered:\n\n0:00 - Introduction.\n0:09 - Training step 1 - Random weights initialization.\n0:20 - Training step 2 - Get predictions.\n0:30 - Training step 3 - Error computation.\n0:45 - Training step 4 - Gradient Descent formula.\n0:56 - Gradient Descent Explained.\n1:25 - Learning Rate Explained.\n1:32 - Do you always find the best solution?\n1:54 - The purpose of the learning rate.\n2:27 - The main types of gradient descent.\n2:40 - Subscribe to our channel!\n\n\ud83d\udd14 Don't forget to like, subscribe, and hit the bell icon to stay updated with our latest videos! \n\n\ud83e\udd16 Note that we use synthetic generations, such as AI-generated images and voices, to enhance the appeal and engagement of our content.\n\n\ud83c\udf10 If you have any questions or topics you want us to cover, leave a comment below. Additionally, share with your thoughts about the content, how do you think we can make them better? Thanks for watching!",
      "thumbnail": "https://i.ytimg.com/vi/CUh4VHpTv1w/mqdefault.jpg",
      "videoId": "CUh4VHpTv1w",
      "publishedAt": "2024-08-20T23:19:26Z",
      "likes": 40,
      "views": 838
    },
    {
      "title": "Overfitting and Underfitting | Bias and Variance Tradeoff in Machine Learning | Clearly Explained!",
      "description": "\ud83d\udd25 Overfitting and Underfitting are two major problems that can be encountered during machine learning model training. Overfitting occurs when your model is more complex than you need and captures the noise of the training data, which is unique to train data only. Meaning, it does not apply to validation, test or other data in the domain. Underfitting happens when your model is to weak to find enough patterns and fails to provide nice results even for the training data.\n\nBias and variance tradeoff is the main concept behind it. You should always control the complexity of your model, as well as other factors influencing the bias and variance tradeoff, which decides whether your model overfits, underfits or generalizes well. \n\nRemember with any train data, you can select very complex model that can overfit and provide the best result on the train set. However, the main goal is to learn general patterns that apply to other data as well.\n\n\ud83d\udd0d Key points covered:\n\n0:00 - Introduction.\n0:14 - Underfitting.\n0:30 - Overfitting.\n0:53 - Two different scenarios for overfitting.\n1:19 - Best case.\n1:29 - Bias and variance tradeoff.\n2:03 - Subscribe to us!\n\n\ud83d\udd14 Don't forget to like, subscribe, and hit the bell icon to stay updated with our latest videos! \n\n\ud83e\udd16 Note that we use synthetic generations, such as AI-generated images and voices, to enhance the appeal and engagement of our content.\n\n\ud83c\udf10 If you have any questions or topics you want us to cover, leave a comment below. Additionally, share with your thoughts about the content, how do you think we can make them better? Thanks for watching!",
      "thumbnail": "https://i.ytimg.com/vi/pptU3bpJojo/mqdefault.jpg",
      "videoId": "pptU3bpJojo",
      "publishedAt": "2024-06-19T14:35:51Z",
      "likes": 43,
      "views": 1306
    },
    {
      "title": "The Role of Validation Sets in Model Training | Train-Test-Validation Splits | Clearly explained!",
      "description": "\ud83d\udd25 In this video we referred to the validation set, a proportion from the overall dataset that has a very significant role! Validation dataset is used for final model selection and hyperparameter tuning, as well as to understand whether your model learns patterns or just overfits the training data. It gives a rough estimate of the performance of the model on an \"unseen\" data. \n\nRemember to use test dataset for final evaluation. You can't use the results from the validation set only, as you used its feedback to tune your hyperparameters and select the best model! \n\n\ud83d\udd0d Key points covered:\n\n0:00 - Introduction.\n0:15 - How different data splits are used in the model creation procedure?\n0:41 - How we define the validation set?\n0:52 - How is validation different from test and train?\n1:00 - What if you evaluate the model based on the validation set?\n1:12 - How is validation data used during the training?\n1:33 - At what point the validation performance will start declining?\n1:48 - How you select the best model based on the validation results?\n1:54 - How to evaluate the final performance?\n1:59 - The size of the validation set.\n2:10 - Subscribe to us!\n\n\ud83d\udd14 Don't forget to like, subscribe, and hit the bell icon to stay updated with our latest videos! \n\n\ud83e\udd16 Note that we use synthetic generations, such as AI-generated images and voices, to enhance the appeal and engagement of our content.\n\n\ud83c\udf10 If you have any questions or topics you want us to cover, leave a comment below. Additionally, share with your thoughts about the content, how do you think we can make them better? Thanks for watching!",
      "thumbnail": "https://i.ytimg.com/vi/NzHE-uNirII/mqdefault.jpg",
      "videoId": "NzHE-uNirII",
      "publishedAt": "2024-06-16T21:52:59Z",
      "likes": 35,
      "views": 646
    },
    {
      "title": "The Purpose of Train-Test Split in Machine Learning | How to Correctly Split Data?",
      "description": "\ud83d\udd25 In this video we discuss one of the most important concepts in machine learning called: train-test split. Through clear visualizations, we explain the significance of splitting the data into train and test sets and how separate subsets should be properly used. Test data is used for final model evaluation, meaning that it should not be used in any other stage. Validation set is used for model selection and configuration, which is a topic we will talk about in the upcoming videos.\n\n\ud83d\udd0d Key points covered:\n\n0:00 - Introduction.\n0:07 - What if you use all data for training?\n0:35 - Why data splits are used?\n0:41 - The ratio of test/train split.\n0:47 - The intuition for splitting the data.\n1:05 - Randomly splitting.\n1:16 - Little data can be problematic!\n1:35 - At what stage to do the train-test split?\n1:55 - What about the validation set?\n2:07 - Subscribe to us!\n\n\ud83d\udd14 Don't forget to like, subscribe, and hit the bell icon to stay updated with our latest videos! \n\n\ud83e\udd16 Note that we use synthetic generations, such as AI-generated images and voices, to enhance the appeal and engagement of our content.\n\n\ud83c\udf10 If you have any questions or topics you want us to cover, leave a comment below. Additionally, share with your thoughts about the content, how do you think we can make them better? Thanks for watching!",
      "thumbnail": "https://i.ytimg.com/vi/4YAq-vCDnKk/mqdefault.jpg",
      "videoId": "4YAq-vCDnKk",
      "publishedAt": "2024-06-13T10:23:10Z",
      "likes": 40,
      "views": 727
    },
    {
      "title": "Checking The Assumptions Of Linear Regression | Statistical And Visual Methods | Part 2",
      "description": "\ud83d\udd25 In this video, we provide the methods for checking the assumptions of linear regression. As mentioned by our latest video, there are four main assumptions of linear regression: Linearity, Independence, Homoscedasticity and Normality. We provide one statistical and one visual way for inspecting each of the assumptions.\n\nTests and methods used in the video for identifying the assumptions are the following:\n\nResiduals vs. Fitted Values plot, Autocorrelation Function Plot (ACF), QQ-Plot, Histogram, Rainbow Test, Ljung-Box Test, Breusch-Pagan Test, Shapiro-Wilk Test.\n\nThe video is introductory. If you wish to learn deeper about each method, tell us in the comments section!\n\n\ud83d\udd0d Key points covered:\n\n0:00 - Introduction.\n0:08 - How to check linearity? (Visual method)\n0:21 - How to check linearity? (Statistical method)\n0:51 - How to check Independence? (Visual method)\n1:06 - How to check Independence? (Statistical method)\n1:18 - How to check Homoscedasticity? (Visual method)\n1:34 - How to check Homoscedasticity? (Statistical method)\n1:57 - How to check Normality? (Visual method)\n2:11 - How to check Normality? (Statistical method)\n2:23 - Subscribe to us!\n\n\ud83d\udd14 Don't forget to like, subscribe, and hit the bell icon to stay updated with our latest videos! \n\n\ud83e\udd16 Note that we use synthetic generations, such as AI-generated images and voices, to enhance the appeal and engagement of our content.\n\n\ud83c\udf10 If you have any questions or topics you want us to cover, leave a comment below. Additionally, share with your thoughts about the content, how do you think we can make them better? Thanks for watching!",
      "thumbnail": "https://i.ytimg.com/vi/wiuhCe9D05o/mqdefault.jpg",
      "videoId": "wiuhCe9D05o",
      "publishedAt": "2024-06-10T18:02:16Z",
      "likes": 30,
      "views": 931
    },
    {
      "title": "Assumptions Of Linear Regression | What To Do If The Assumptions Do Not Hold? | Part 1",
      "description": "\ud83d\udd25 The video talks about the assumptions of the linear regression. There are four main assumptions that should hold for the statistical properties to be valid. Additionally, we discuss what to do if the assumptions do not hold? What if the real-world datasets are too complex for the assumptions to hold? \nSplitting the dataset into train and test datasets and using them for model evaluation can be a good way to validate the performance and reliability of the linear regression model, however, you should be very careful before relying on the statistical properties, if there are violations.\n\nStay tuned to learn about the way you can check the assumptions, as well as about how to effectively apply the train-test split approach.\n\n\ud83d\udd0d Key points covered:\n\n0:00 - Introduction.\n0:18 - 1. Linearity\n0:35 - 2. Independence\n0:52 - 3. Homoscedasticity\n1:05 - 4. Normality\n1:18 - What if the assumptions do not hold?\n1:32 - Resolve the violation.\n1:44 - Train-Test Split Approach.\n2:15 - Subscribe to us!\n\n\ud83d\udd14 Don't forget to like, subscribe, and hit the bell icon to stay updated with our latest videos! \n\n\ud83e\udd16 Note that we use synthetic generations, such as AI-generated images and voices, to enhance the appeal and engagement of our content.\n\n\ud83c\udf10 If you have any questions or topics you want us to cover, leave a comment below. Additionally, share with your thoughts about the content, how do you think we can make them better? Thanks for watching!",
      "thumbnail": "https://i.ytimg.com/vi/XTAdmNj2rCY/mqdefault.jpg",
      "videoId": "XTAdmNj2rCY",
      "publishedAt": "2024-06-06T23:01:44Z",
      "likes": 39,
      "views": 724
    },
    {
      "title": "Linear Regression Explained | A Beginner's Guide To Regression | The Basics You Need to Know!",
      "description": "\ud83d\udd25 Linear Regression is a statistical and the simplest regression model used for finding the linear relationship between a dependent and one or more independent variables. The goal is to find the best-fitting straight line through a set of data points. The video aims to explain the concept of linear regression by understanding the linear functions. The visuals and learning procedure presented in the video further strengthen the understanding of the main concept.\n\nThere are still important points about linear regression that will be addressed in the upcoming videos, so follow us not to miss them!\n\n\ud83d\udd0d Key points covered:\n\n0:00 - What is Linear Regression?\n0:25 - Introduction to the House Price Prediction Example.\n0:34 - Linear functions.\n0:54 - Simplest example of Linear Regression.\n1:15 - Weights and Bias.\n1:38 - The procedure of finding the correct weights.\n2:02 - Visualization of the learning procedure.\n2:37 - Subscribe to us!\n\n\ud83d\udd14 Don't forget to like, subscribe, and hit the bell icon to stay updated with our latest videos! \n\n\ud83e\udd16 Note that we use synthetic generations, such as AI-generated images and voices, to enhance the appeal and engagement of our content.\n\n\ud83c\udf10 If you have any questions or topics you want us to cover, leave a comment below. Additionally, share with your thoughts about the content, how do you think we can make them better? Thanks for watching!",
      "thumbnail": "https://i.ytimg.com/vi/qyeB4j80WTc/mqdefault.jpg",
      "videoId": "qyeB4j80WTc",
      "publishedAt": "2024-06-02T19:26:36Z",
      "likes": 78,
      "views": 3013
    },
    {
      "title": "The Ultimate Guide To Supervised Learning | Classification And Regression | Part 2",
      "description": "\ud83d\udd25 The second part of the ultimate guide to supervised learning talks about the two types of supervised algorithms: classification and regression. The main differences are explained on real-world examples and visuals. Additionally, the algorithms and evaluation metrics for both categories are listed. Note that we will talk about them in the upcoming videos! So, stay tuned!\n\n\n\ud83d\udd0d Key points covered:\n\n0:00 - Introduction.\n0:05 - Classification explained.\n0:17 - Binary classification explained.\n0:34 - Multiclass classification explained.\n0:48 - Regression explained.\n1:03 - Regression examples.\n1:09 - ML algorithms for classification and regression.\n1:16 - Evaluation metrics for classification and regression.\n1:28 - Subscribe to us!\n\n\ud83d\udd14 Don't forget to like, subscribe, and hit the bell icon to stay updated with our latest videos! \n\n\ud83e\udd16 Note that we use synthetic generations, such as AI-generated images and voices, to enhance the appeal and engagement of our content.\n\n\ud83c\udf10 If you have any questions or topics you want us to cover, leave a comment below. Additionally, share with your thoughts about the content, how do you think we can make them better? Thanks for watching!",
      "thumbnail": "https://i.ytimg.com/vi/DfjLmdQWSZo/mqdefault.jpg",
      "videoId": "DfjLmdQWSZo",
      "publishedAt": "2024-05-31T22:06:06Z",
      "likes": 28,
      "views": 791
    },
    {
      "title": "The Ultimate Guide To Supervised Learning | Explained On Binary Classification Example | Part 1",
      "description": "\ud83d\udd25 The first part of \"The Ultimate Guide To Supervised Learning\" explains the concept of supervised learning on an example of Titanic survival dataset. The example is a binary classification task which aims to predict if a passenger would survive the accident or not. The video aims to explain the learning process of the supervised algorithms in high-level to grasp intuition of how the overall pipeline works. \n\nThe second part will cover the types of supervised learning models and other important points.\n\n\ud83d\udd0d Key points covered:\n\n0:00 - What is supervised learning? (recap).\n0:14 - Example of the Titanic survival dataset.\n0:36 - Understand the concept at high-level.\n1:19 - Understand the concept at low-level.\n3:27 - Subscribe to us!\n\n\ud83d\udd14 Don't forget to like, subscribe, and hit the bell icon to stay updated with our latest videos! \n\n\ud83e\udd16 Note that we use synthetic generations, such as AI-generated images and voices, to enhance the appeal and engagement of our content.\n\n\ud83c\udf10 If you have any questions or topics you want us to cover, leave a comment below. Additionally, share with your thoughts about the content, how do you think we can make them better? Thanks for watching!",
      "thumbnail": "https://i.ytimg.com/vi/gC2rgECJMZY/mqdefault.jpg",
      "videoId": "gC2rgECJMZY",
      "publishedAt": "2024-05-30T20:30:36Z",
      "likes": 32,
      "views": 1035
    },
    {
      "title": "Best AI Music Generator | Music Generation Tool for FREE | MusicGen developed by Meta AI",
      "description": "\ud83d\udd25 This video talks about one of the recent music generation models designed by Meta AI called MusicGen. MusicGen uses the recent advancements in the music generation domain to produce high-quality, good-sounding music in various genres, moods, and instruments. \n\nYou can use MusicGen in two ways:\n1) Through the official website, but you need to buy credits for more uses.\n2) Using the official repository and running the codes manually on your device. \n\nThe steps for configuring the libraries necessary for running the model and the Jupyter Notebook can be downloaded using the following link:\n\nhttps://drive.google.com/drive/folders/1Dmg9IL_AKirZBW-c49qHtz866lDIoLna\n\nNote 1: You need at least 3 GB of GPU memory or a powerful CPU for generating with a \"small\" model. Be sure you don't run it on a very weak system. \n\nNote 2: The installation can be a bit complex. If you need assistance, feel free to ask them in the comments section!\n\n\ud83d\udd0d Key points covered:\n\n0:00 - Introduction to MusicGen.\n0:07 - An example generation.\n0:25 - Why MusicGen?\n0:40 - The first way to use the model. (With credits)\n1:32 - The second way to use the model. (Totally free)\n2:28 - Subscribe to us!\n\n\ud83d\udd14 Don't forget to like, subscribe, and hit the bell icon to stay updated with our latest videos! \n\n\ud83e\udd16 Note that we use synthetic generations, such as AI-generated images and voices, to enhance the appeal and engagement of our content.\n\n\ud83c\udf10 If you have any questions or topics you want us to cover, leave a comment below. Additionally, share with your thoughts about the content, how do you think we can make them better? Thanks for watching!",
      "thumbnail": "https://i.ytimg.com/vi/mMkCn2UimLA/mqdefault.jpg",
      "videoId": "mMkCn2UimLA",
      "publishedAt": "2024-05-27T14:11:49Z",
      "likes": 39,
      "views": 3873
    },
    {
      "title": "Types Of Machine Learning Algorithms | Explained On Real World Examples | ML For Beginners",
      "description": "\ud83d\udd25 Traditional types of Machine Learning include: Supervised Learning, Unsupervised learning and Reinforcement learning. There are also Semi-supervised algorithms which are out of the scope of this video. We aim to provide a fundamental understanding of all the categories by explaining their use cases and characteristics. After watching the video, you should be able to identify ML tasks into their corresponding types.\n\nTo sum up the whole idea:\n\nSupervised Learning - When you have \"label\" you want to \"predict\" with your model in the dataset.\n\nUnsupervised Learning - When you don't have the \"label\" and want to group or cluster (there are also other types like dimensionality reduction, which is a more complex topic) the dataset, without knowing what should be the exact output of the model.\n\nReinforcement learning - Includes learning through trial and error in the environment by maximizing the reward function. \n\n\ud83d\udd0d Key points covered:\n\n0:00 - The main types of ML algorithms.\n0:15 - Supervised learning.\n0:47 - Unsupervised learning.\n1:26 - Reinforcement learning.\n2:05 - Subscribe to us!\n\n\ud83d\udd14 Don't forget to like, subscribe, and hit the bell icon to stay updated with our latest videos! \n\n\ud83e\udd16 Note that we use synthetic generations, such as AI-generated images and voices, to enhance the appeal and engagement of our content.\n\n\ud83c\udf10 If you have any questions or topics you want us to cover, leave a comment below. Additionally, share with your thoughts about the content, how do you think we can make them better? Thanks for watching!",
      "thumbnail": "https://i.ytimg.com/vi/ZNrwlu7cvsI/mqdefault.jpg",
      "videoId": "ZNrwlu7cvsI",
      "publishedAt": "2024-05-23T21:21:09Z",
      "likes": 72,
      "views": 1969
    },
    {
      "title": "AI vs. ML vs. DL vs. DS - Difference Explained | On Real World Examples | AI For Beginners",
      "description": "\ud83d\udd25 Artificial Intelligence, Machine Learning, Deep Learning and Data Science, what are the differences? The video goes deep into the unique characteristics and practical applications of each domain by highlighting specific examples. Having a clear understanding of the distinctions is a crucial knowledge in the field of AI. Don't miss out this valuable information!\n\nNote that some points discussed are not very detailed. There are still some important attributes that need careful attention. If you want to know more about Artificial Intelligence, subscribe to our channel as we are going to explain all the fundamentals in easy-to-understand manner!\n\n\ud83d\udd0d Key points covered:\n\n0:00 - Introduction.\n0:08 - What is Artificial Intelligence?\n0:23 - How is AI different from all?\n1:04 - What is Machine Learning?\n1:17 - What is unique about ML?\n1:38 - What is Deep Learning and what is unique about it?\n2:14 - How is Data Science different from AI?\n\n\ud83d\udd14 Don't forget to like, subscribe, and hit the bell icon to stay updated with our latest videos! \n\n\ud83e\udd16 Note that we use synthetic generations, such as AI-generated images and voices, to enhance the appeal and engagement of our content.\n\n\ud83c\udf10 If you have any questions or topics you want us to cover, leave a comment below. Additionally, share with your thoughts about the content, how do you think we can make them better? Thanks for watching!",
      "thumbnail": "https://i.ytimg.com/vi/hhFG9X3h8t8/mqdefault.jpg",
      "videoId": "hhFG9X3h8t8",
      "publishedAt": "2024-05-21T14:24:47Z",
      "likes": 261,
      "views": 3665
    },
    {
      "title": "Artificial Intelligence Explained In Simple Words | What Is AI? | Explained On A Real World Example!",
      "description": "#aiexplained #ai #artificialintelligence #machinelearning  #easy #realworld \n\ud83d\udd25 Artificial Intelligence explained in simple words on a real world example! By going through the example of house price prediction, the video aims to explain how a simple AI house price prediction algorithm works. By comparing the intelligence of humans with AI, we assess how closely human intelligence stands to the mathematical algorithms in this context.\n\nSome details and points are abstract and not detailed. If you want to know more about how AI works, subscribe to our channel since we are going to explain all the basics in easy-to-understand manner!\n\n\ud83d\udd0d Key points covered:\n\n0:00 - Introduction to the problem.\n0:11 - Why are brokers that good?\n0:22 - Criteria for comparison (Role of features).\n0:32 - The role of Statistics.\n0:48 - What are AI models?\n0:56 - AI for house price prediction.\n1:38 - Subscribe to us!\n\n\ud83d\udd14 Don't forget to like, subscribe, and hit the bell icon to stay updated with our latest videos! \n\n\ud83e\udd16 We break down complex AI concepts into easy-to-understand videos, perfect for beginners and enthusiasts alike. Whether you're curious about machine learning, neural networks, or the future of AI, you'll find engaging and informative content here.\n\n\ud83d\udcda Note that we use synthetic generations, such as AI-generated images and voices, to enhance the appeal and engagement of our content.\n\n\ud83c\udf10 If you have any questions or topics you want us to cover, leave a comment below. Additionally, share with your thoughts about the content, how do you think we can make them better? Thanks for watching!",
      "thumbnail": "https://i.ytimg.com/vi/vUTEoEOBN-I/mqdefault.jpg",
      "videoId": "vUTEoEOBN-I",
      "publishedAt": "2024-05-16T20:55:31Z",
      "likes": 54,
      "views": 2187
    }
  ]
}